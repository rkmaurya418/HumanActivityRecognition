---
title: "Course Project"
author: "Ritesh Kumar Maurya"
date: "03/05/2020"
output: html_document
---
Data source: http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har

Step1: Loading required librares

```{r}
library(caret)
library(ggplot2)
library(rlist)
library(e1071)
library(randomForest)
```

Step:2
---
-> Loading training data set
---
-> Running through the dataset a lot of missing values were observed, so they were cleaned, by identifying rows with missing values
---
-> Rows 1:7 were also removed as they were information related to user and time stamp, in no form they relate to classe
---
This results in a training dataset with 53 rows

```{r}
training = read.csv("E:/Practical Machine Learning/Prediction Assignment/pml-training.csv",
                    na.strings = c("","NA"))
#variable to store missing value columns
col <- NULL
#checking lissing value columns
for (i in 1:dim(training)[2])
  {
  if (sum(is.na(training[,i]))>0)
  {
   col <- c(col,i)
  }
}
#final training set
training <- training[,-col]
training <- training[,-c(1:7)]
dim(training)

```

Step:3 
---
We try to check correlations between variables.
---
These plots show us very little direct correlations between classe and predictor variables.

```{r}
featurePlot(training[,1:4], training[,53],"pairs" )
featurePlot(training[,5:8], training[,53],"pairs" )
featurePlot(training[,8:12], training[,53],"pairs" )
```

Step:4
---
Training set is the only set we have got with classe information.
---
so we will split it into subsets of training and validation.

```{r}
inSubTrain = createDataPartition(y=training$classe,
                           p=0.6,
                           list=FALSE)
SubTrain = training[inSubTrain,]
SubTest = training[-inSubTrain,]

training <- SubTrain
validation <- SubTest

```

Step:5
---
Since we could not find any very direct corelations so we will use all available variables and hope to go a good model.
---
Also, we will try to get accurate model with svm first then will try random forest.
---
Going through the paper on human activity recognition few researchers got very good accuracy by combining svm and naive bayes model

```{r}
mod1 <- svm(classe ~ .,
              data=training)
mod2 <- randomForest(classe ~ .,
              data= training,
              method="class")
```

Step:6
---
Testing training set accuract and table on both the models
---
We observe randomForest is performing fantastic but this could also be because of overfitting
---
so  we will test these models for validation set as well and check how they are performing

```{r}

PredTrainMod1 <- predict(mod1, training)
PredTrainMod2 <- predict(mod2, training, type="class")

print("SVM model on training")
confusionMatrix(PredTrainMod1, training$classe)$overall[1]
table(PredTrainMod1, training$classe)

print("Random forest model on training")
confusionMatrix(PredTrainMod2, training$classe)$overall[1]
table(PredTrainMod2, training$classe)


PredValidationMod1 <- predict(mod1, validation)
PredValidationMod2 <- predict(mod2, validation, type="class")

print("SVM model on validation")
confusionMatrix(PredValidationMod1, validation$classe)$overall[1]
table(PredValidationMod1, validation$classe)

print("Random forest model on validation")
confusionMatrix(PredValidationMod2, validation$classe)$overall[1]
table(PredValidationMod2, validation$classe)
```

This shows that random forest is still performing better than SVM.
---
So we will select random forest as final model.
---
Step:7
---
Now we will apply same data treatment to testing set and run it on random forest model (mod2) to obtain our estimations on test data
---
```{r}
testing = read.csv("E:/Practical Machine Learning/Prediction Assignment/pml-testing.csv",
                   na.strings = c("","NA"))
col <- NULL

for (i in 1:dim(testing)[2])
{
  if (sum(is.na(testing[,i]))>0)
  {
    col <- c(col,i)
  }
}

testing <- testing[,-col]
testing <- testing[,-c(1:7)]
dim(testing)

 
PredTestMod2 <- predict(mod2, testing, type="class")
PredTestMod2


```